# Climbing trees with UI Automation

We all think we're pretty clever when it comes to adding unit and integration tests, right? Most of us can agree it's a solid safety net to ensure that we're building is right. So, when our code doesn't seem broken, so we'll try out whatever we've built: click on a few things, press a few buttons, maybe type a thing or two, just to make sure what we've built works. A little manual testing can go a long way to ensure what we've built is looking good and working the way we visually intended it to. Here's a few things we try to achieve when we test manually:

1. Fixing a bug - We're making sure the code we just wrote fixes the bug
2. Added a new feature - make sure it looks the way we wanted to and also works in general
3. Final release walk through - Just a few checks to make sure everything we did looks good

But how extensive is that manual testing? The common thing here is that these scenarios have laser focused goals in mind. In other words, it's very easy to miss other bugs when you're focused on replicating the bug fix or specific features you're testing. This can eventually lead to a "whack a mole" type of process of squashing regressive bugs. Added on to the other bugs that are left to fix, it's easy to get overwhelmed. This was the problem our small team was facing when making new releases.

One obvious solution to this problem is to add a manual tester to our team and quality process, but with a small number of quality engineers, that number being one, who understands how the extension works and had quality responsibilities in other teams. We had a quality engineer with some automation experience, who built an automation framework, which seemed like a great idea, but at the time we needed manual testing the most, and there was also some technical humps that made the automation kind of rough, so the project was initially abandoned and the a few months later that quality engineer moved on to another team.

Our technical project manager at the time introduced us to the Coded UI library and showed us a small demo to our team. We were impressed and interested in its place in our quality process, and I was tasked with figuring that out, and so I started my attempt by picking apart Sara's demo and building on it.

Microsoft introduced Coded UI into Visual Studio Enterprise which includes a library and tools built into VS to for tests driven through the user interface, hence the term Coded UI. Coded UI supports Win Form and WPF applications, web applications and services, and Windows store and phone applications. We're using WPF for our extension. We have some flexibility on how we want to build this test suite. The first option is through the MSFT test manager with its Coded UI Record and Playback feature in Visual Studio. With this, you'd hit the record button and record some user actions, and generate some code, including a test case. Alternatively there's the option of hand coding test cases, this provides a little more flexibility in how the test cases created, providing the opportunity to create a large framework for automation and tests. We took the manual code route. The extension is rather large and has lots of little details a features, including extending the Team Explorer window. We wanted to tailor the test cases and automated steps for clarity and reuse. This leads us to the page object model, which we adopted.

Generally, the page object model is associated with Selenium, another automation library that is specifically used for web applications. In the most general terms, the page object model encapsulates all UI element access and alterations (like writing text in a text box) and rolls it up into an API, with methods for specific actions. Test cases call these methods and playback those target actions. In the API These methods may or may not return an object that can be used to drill further down into some additional actions. When the actions are finally completed in a test case or if we want to verify the UI state before continuing on, we can add some assertions along the way. In the page object model the page usually refers to an HTML page and the elements within it, but for us it's the current view in our extension and the controls within it.

Our pull request list view would be an example of this. We'll get into a repo we know has no Pull Requests, and make sure there's no pull request items in the view.

One of the biggest challenges and critical things about our extension is that we're the only third party extension that ships with Visual Studio 2015 and 2017. When you install our extension, you'll notice we're embedded in Team Explorer, which is one of the few entry points to our features. We also have some repository independent features that are available, like creating gists from your code. With that said, you don't always need to be working in a GitHub hosted repo to use some of our features. Therefore, it's imperative that we avoid diminishing a developer's experience with our extension installed. So things like crashing, slowing down, or preventing access VS specific features are things we really try to avoid when we cut a new release. One of the first test cases that was made was testing our features in Team Explorer, to make sure the extension is installed correctly and confirm that the extension is being exposed when in a GitHub hosted repository. So, when we run a test case, the first thing we do is launch an experimental instance of Visual Studio. CUI has a built in "Launch" method to do this.

To access a control, we need to tell CUI what to look for, we need provide some identifiable properties of the control we're looking for. Things like its name, or maybe what kind of control it is, or maybe it was given a static GUID that we can use to find it. CUI calls them search properties. With these search properties CUI uses a method called Find(). This gets implicitly called when call do an action on it, but will throw an exception if not found. This can make the testing experience a little choppy, so instead of letting Find happen during an action, we use WPF control specific methods `Exists()` and optionally `Enabled()` methods to confirm it's there. In early exploration of Coded UI, we had to identify these control properties, and we used the Inspect tool to do this. We also gave our controls specific attributes called automation ids in WPF, which are unique IDs to assist with the search. In some cases, we can find a control with that automation id alone.

Coded UI uses a breadth-first search algorithm to find controls. Without getting to much into traversing undirected graphs, here's some basic takeaways for how it's done for us in Coded UI. BFS requires a source or predecessor vertex, which in our case is some control that is the parent of the control we're searching for. With given search properties, Coded UI searches for the closest thing to those properties. Coded UI doesn't require all the properties of a control to find it, it helps narrow it down though if there's multiple controls with similar properties where Coded UI might return something you didn't want.

GHfVS has a list of manual steps that are done during the testing process. For now, we extract our test cases from there. It's important to note here that while UI automation is a pretty good way to get some manual testing out of the way, this shouldn't be a catch all, nor a replacement to manual testing. Just a supplement.

UI automation can be good for regressions, and catching visual discrepancies in your application. What else could you do? One thing that comes to mind is investigating the use of a feature. We maintain an internal usage metrics on the use of our extension from those who opt in. If we are seeing a decline in the use of a feature, new or old, we could  automate the steps to find that feature and evaluate things its and how many steps it takes to get to that feature. Also, we know that sometimes nobody bothers to mention your bugs, especially if there's a workaround. Somebody finally mentions it in passing so you want to investigate. Automation can help uncover those bugs or errors that you're not aware of. You could tell Coded UI to take screenshots for you at any given time to gain some visual insight on what's happening.

Our framework is still in development, taking time to tweak some performance issues and add a dash of refactoring here and there, but we have a goal of how we want to continue to integrate the framework. Periodically, either once a week or during a release, we'll have a dedicated VM set up to run through all the tests, alternatively we will manually run subsets of tests for targeted feature testing. We'd start tests and get their results through chat commands. We'd have our QA engineer identify and implement new test cases, as the engineers would implement the feature and extend the framework by adding additional actions so the feature can be used in test cases.

With the help of Coded UI, over 60% of our extension features are now automated. Other than the hope to eventually have at least 90% of automated feature coverage, we hope to continue to find new ways to integrate this into our quality process, specifically when we're ready to cut a new release, when we need to test the stability of our master/baseline branch, and when we need to simulate targeted user scenarios that lead to bugs or cosmetic errors that oversights from manual testing or can't be tested with unit tests. We also hope that when our next quality engineer joins our team, that they can easily add additional test cases with clean and well documented steps to automate it. Finally, we want to make this open source, just like the extension and have it as a reference point for others who are looking to automate their desktop applications, and for those who want to contribute.

UI testing might not be for you, and you may not need it. But if you do, or if you're interested in how we've done it I hope you'll check it out when it's public. In the meantime check out the GhfVS repo and Unity repos!
